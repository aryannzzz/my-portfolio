# Project GRASP — Foundation Models for Robotic Control

**Timeline:** May 2025 – Present  
**Lab:** SenAI Lab, IIT Madras  
**Tech Stack:** Python, PyTorch, Transformers, OpenCV, ROS, CLIP

## Overview

Developing vision-language-action (VLA) models that combine visual perception, natural language understanding, and robotic control for generalizable manipulation tasks.

## Research Focus

### 1. **Vision-Language-Action (VLA) Models**
- Combining CLIP-based encoders with transformer policies
- Multimodal alignment for visuomotor tasks
- Behavior cloning from human demonstrations

### 2. **In-Context Learning for Robotics**
- **RoboPrompt** — Few-shot learning for robotic tasks
- Object-centric representations for scene understanding
- Adaptive control in unseen environments

### 3. **Key Technical Areas**

#### GradCAM Integration with RL
- Visual attention mechanisms for policy interpretability
- Saliency maps for understanding robot decision-making
- Currently: Developing integration framework

#### VLA Architectures
- **MoleVLA** — Molecular-level action representations
- **SmolVLA** — Lightweight VLA for edge deployment
- **ATK (Action Tokenization Kernel)** — Novel action encoding

#### Dataset Work
- **SO-101 Dataset** — Standard objects manipulation
- Teleoperation systems for data collection
- Large-scale demonstration datasets

## Technical Achievements

### Sim-to-Real Transfer
- Diffusion-based policy generation
- Domain randomization techniques
- Reality gap bridging strategies

### Object-Centric Representations
- Decomposing scenes into objects
- Compositional understanding for manipulation
- Transfer learning across object categories

## Research Contributions

- Literature review on foundation models for robotics
- Explored latest VLA architectures (MoleVLA, SmolVLA)
- Investigated in-context learning paradigms
- Studied behavior cloning methodologies

## Experimental Work

- Building teleoperation interfaces
- Collecting manipulation datasets
- Training transformer-based policies
- Evaluating sim-to-real performance

## Future Directions

- Scaling up VLA models with larger datasets
- Real robot deployment and validation
- Multi-task learning frameworks
- Long-horizon task planning

## Impact

- Selected for competitive robotics research program
- Contributing to cutting-edge VLA research
- Building towards generalizable robotic manipulation

---

**Lab:** SenAI Lab, IIT Madras  
**Coordinator:** iBot Club CV & RL Wing  
**Status:** Active research project
